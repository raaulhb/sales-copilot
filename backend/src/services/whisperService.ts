import { openai, AI_CONFIG } from "../config/ai";
import fs from "fs";
import path from "path";
import { promisify } from "util";

const writeFile = promisify(fs.writeFile);
const unlink = promisify(fs.unlink);

interface WhisperResult {
  transcript: string;
  confidence: number;
  language: string;
  duration: number;
  segments?: any[];
}

class WhisperService {
  async transcribeAudio(
    audioBuffer: Buffer,
    filename: string = "audio.wav"
  ): Promise<WhisperResult> {
    // Check if we should use real AI
    const useRealAI =
      process.env.USE_REAL_AI === "true" && process.env.OPENAI_API_KEY;

    if (!useRealAI) {
      console.log(
        "ü§ñ Using mock transcription (USE_REAL_AI=false or no API key)"
      );
      return this.mockTranscription(audioBuffer);
    }

    try {
      // Create temporary file for Whisper API
      const tempDir = path.join(__dirname, "../../temp");
      if (!fs.existsSync(tempDir)) {
        fs.mkdirSync(tempDir, { recursive: true });
      }

      const tempFilePath = path.join(
        tempDir,
        `whisper-${Date.now()}-${filename}`
      );

      // Write buffer to temp file
      await writeFile(tempFilePath, audioBuffer);

      console.log(
        `üé§ Transcribing with Whisper: ${audioBuffer.length} bytes ‚Üí ${tempFilePath}`
      );

      // Call OpenAI Whisper API
      const transcription = await openai.audio.transcriptions.create({
        file: fs.createReadStream(tempFilePath),
        model: AI_CONFIG.whisper.model,
        language: AI_CONFIG.whisper.language,
        response_format: "verbose_json",
        temperature: AI_CONFIG.whisper.temperature,
      });

      // Clean up temp file
      await unlink(tempFilePath);

      console.log("‚úÖ Whisper transcription completed:", transcription.text);

      return {
        transcript: transcription.text || "",
        confidence: 0.95, // Whisper doesn't provide confidence
        language: transcription.language || "pt",
        duration: transcription.duration || 0,
        segments: (transcription as any).segments || [],
      };
    } catch (error: any) {
      console.error("‚ùå Whisper transcription error:", error.message);

      // Fallback to mock on error
      console.log("ü§ñ Falling back to mock transcription");
      return this.mockTranscription(audioBuffer);
    }
  }

  private mockTranscription(audioBuffer: Buffer): WhisperResult {
    // Generate realistic mock transcription
    const mockTexts = [
      "Ol√°, como posso ajud√°-lo hoje?",
      "Estou interessado em saber mais sobre seus produtos.",
      "Qual √© o pre√ßo do seu servi√ßo?",
      "Preciso entender melhor os benef√≠cios.",
      "Quando podemos agendar uma demonstra√ß√£o?",
      "Hmm, interessante. Mas eu preciso analisar os n√∫meros.",
      "Parece uma boa solu√ß√£o para nossa empresa.",
      "Qual √© o prazo de implementa√ß√£o?",
      "Voc√™s oferecem suporte t√©cnico?",
      "Gostaria de discutir isso com minha equipe.",
    ];

    const randomText = mockTexts[Math.floor(Math.random() * mockTexts.length)];

    return {
      transcript: randomText,
      confidence: 0.85 + Math.random() * 0.1, // 85-95%
      language: "pt",
      duration: 3.0,
      segments: [],
    };
  }

  async isConfigured(): Promise<boolean> {
    return !!(process.env.OPENAI_API_KEY && process.env.USE_REAL_AI === "true");
  }
}

export const whisperService = new WhisperService();
