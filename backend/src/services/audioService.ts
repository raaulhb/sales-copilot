import { ConversationSession, AudioSegment, AudioFeatures } from "../types";
import { v4 as uuidv4 } from "uuid";
import { whisperService } from "./whisperService";
// Temporary in-memory storage for demo
const sessions: Map<string, ConversationSession> = new Map();
const audioSegments: Map<string, AudioSegment[]> = new Map();

interface CreateSessionData {
  userId: string;
  clientName?: string;
  clientCompany?: string;
}

interface ProcessAudioData {
  sessionId: string;
  audioData: Buffer;
  timestamp: number;
  filename?: string;
}

class AudioService {
  async createSession(data: CreateSessionData): Promise<ConversationSession> {
    const sessionId = uuidv4();

    const session: ConversationSession = {
      id: sessionId,
      userId: data.userId,
      clientName: data.clientName,
      clientCompany: data.clientCompany,
      startTime: new Date(),
      status: "active",
      audioSegments: [],
    };

    sessions.set(sessionId, session);
    audioSegments.set(sessionId, []);

    console.log(`üìû New session created: ${sessionId}`);
    return session;
  }

  async getSession(sessionId: string): Promise<ConversationSession | null> {
    const session = sessions.get(sessionId);
    if (!session) return null;

    // Get latest audio segments
    const segments = audioSegments.get(sessionId) || [];
    session.audioSegments = segments;

    return session;
  }

  async endSession(sessionId: string): Promise<ConversationSession | null> {
    const session = sessions.get(sessionId);
    if (!session) return null;

    session.status = "completed";
    session.endTime = new Date();

    sessions.set(sessionId, session);
    console.log(`üìû Session ended: ${sessionId}`);

    return session;
  }

  async processAudioSegment(data: ProcessAudioData): Promise<AudioSegment> {
    const { sessionId, audioData, timestamp, filename } = data;

    console.log(`üéôÔ∏è Processing audio segment: ${audioData.length} bytes`);

    try {
      // Real transcription with Whisper
      const transcriptionResult = await whisperService.transcribeAudio(
        audioData,
        filename
      );

      // Mock audio processing - will be replaced with real implementation
      const mockAudioFeatures: AudioFeatures = {
        pace: Math.floor(Math.random() * 100) + 100, // 100-200 WPM
        volume: Math.floor(Math.random() * 100),
        pitch: Math.floor(Math.random() * 200) + 100, // 100-300 Hz
        energy: Math.floor(Math.random() * 100),
        pauseFrequency: Math.random() * 10,
        emotionalTone: ["positive", "neutral", "negative", "mixed"][
          Math.floor(Math.random() * 4)
        ] as any,
      };

      const segment: AudioSegment = {
        id: uuidv4(),
        sessionId,
        timestamp,
        duration: transcriptionResult.duration * 1000, // Convert to ms
        speakerId: Math.random() > 0.5 ? "seller" : "client",
        transcript: transcriptionResult.transcript,
        audioFeatures: mockAudioFeatures,
      };

      // Store segment
      const segments = audioSegments.get(sessionId) || [];
      segments.push(segment);
      audioSegments.set(sessionId, segments);

      console.log(
        `‚úÖ Audio segment processed with transcript: "${transcriptionResult.transcript}"`
      );
      return segment;
    } catch (error) {
      console.error("‚ùå Audio processing error:", error);
      throw error;
    }
  }

  async getAllSessions(userId: string): Promise<ConversationSession[]> {
    return Array.from(sessions.values()).filter(
      (session) => session.userId === userId
    );
  }
}

export const audioService = new AudioService();
